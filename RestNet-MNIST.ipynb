{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "train_data = datasets.MNIST('./data', download=True, transform=transform, train=True)\n",
    "test_data = datasets.MNIST('./data', download=True, transform=transform, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "input_size = 28\n",
    "num_layers = 2\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        # A whole block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        # Time to add the identity\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers will be a list telling how many times to use the block\n",
    "# In ResNet50 layers would be [3,4,6,3]\n",
    "class ResNet(nn.Module): \n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        #Not ResNet layers yet, just initialization\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2,padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        #ResNet layers\n",
    "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512*4, num_classes)\n",
    "    def forward(self, x):\n",
    "        # First part\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # Into the Resnet blocks\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # Putting it through the average pooling to make sure its in a correct shape (1,1)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _make_layer(self, block, num_residual_blocks, out_channels, stride):\n",
    "        identity_downsample = None\n",
    "        # Going to add the layers to a list\n",
    "        layers = []\n",
    "        \n",
    "        # If the stride is not 1 or if the number of channels when the layer is over * 4 \n",
    "        # is not equal to the number of channels inputted initially,\n",
    "        # then the size is changed after going through the block\n",
    "        # Which means we have to change the identity_downsample to match the output\n",
    "        \n",
    "        if stride != 1 or self.in_channels != out_channels * 4:\n",
    "            \n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels*4, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels*4)\n",
    "            )\n",
    "        #This first block is responsable for changing the number of channels\n",
    "        layers.append(block(self.in_channels, out_channels, identity_downsample, stride))    \n",
    "        \n",
    "        self.in_channels = out_channels * 4 #256 channels\n",
    "        \n",
    "        for i in range(num_residual_blocks - 1): # -1 because already computted one block\n",
    "            layers.append(block(self.in_channels, out_channels)) #256, 64 in the second block\n",
    "            \n",
    "        return nn.Sequential(*layers)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that creates ResNet50. Creating ResNet101, for example, would only require to change the layers list values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(img_channels, num_classes):\n",
    "    return ResNet(Block, [3, 4, 6, 3], img_channels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if the output size is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net = ResNet50(1, 10)\n",
    "    x = torch.rand(2, 1, 28, 28)\n",
    "    y = net(x)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(in_channels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, loss_func, optimizer_func, train_loader, num_epochs):\n",
    "    time_epoch = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        time_100_batch = time.time()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            out = model(images)\n",
    "            loss = loss_func(out, labels)\n",
    "            optimizer_func.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i%101==100:\n",
    "                print(f'Epoch:{epoch}, Batch:{i}, Loss:{loss.item()}, Time Spent:{time.time()-time_100_batch}')\n",
    "    print(f'Done Training. Total Time: {time.time()-time_epoch}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Batch:100, Loss:0.18761266767978668, Time Spent:158.4418601989746\n",
      "Epoch:0, Batch:201, Loss:0.17259813845157623, Time Spent:319.6685926914215\n",
      "Epoch:0, Batch:302, Loss:0.1999676525592804, Time Spent:473.0272183418274\n",
      "Epoch:0, Batch:403, Loss:0.22521115839481354, Time Spent:626.6853034496307\n",
      "Epoch:0, Batch:504, Loss:0.2988954782485962, Time Spent:779.2470347881317\n",
      "Epoch:0, Batch:605, Loss:0.1002664864063263, Time Spent:931.3142457008362\n",
      "Epoch:0, Batch:706, Loss:0.012032115831971169, Time Spent:1085.226448059082\n",
      "Epoch:0, Batch:807, Loss:0.09263377636671066, Time Spent:1237.1847155094147\n",
      "Epoch:0, Batch:908, Loss:0.05352942273020744, Time Spent:1390.5446619987488\n",
      "Epoch:1, Batch:100, Loss:0.1358070820569992, Time Spent:163.04352736473083\n",
      "Epoch:1, Batch:201, Loss:0.39713066816329956, Time Spent:316.77297496795654\n",
      "Epoch:1, Batch:302, Loss:0.20646950602531433, Time Spent:469.304594039917\n",
      "Epoch:1, Batch:403, Loss:0.09596759080886841, Time Spent:622.3921656608582\n",
      "Epoch:1, Batch:504, Loss:0.028064705431461334, Time Spent:775.5167257785797\n",
      "Epoch:1, Batch:605, Loss:0.40061455965042114, Time Spent:933.7251043319702\n",
      "Epoch:1, Batch:706, Loss:0.17928090691566467, Time Spent:1088.9984924793243\n",
      "Epoch:1, Batch:807, Loss:0.3478294312953949, Time Spent:1252.7166261672974\n",
      "Epoch:1, Batch:908, Loss:0.025245225057005882, Time Spent:1406.6617178916931\n",
      "Done Training. Total Time: 2885.6959195137024\n"
     ]
    }
   ],
   "source": [
    "train(model, criterion, optimizer, train_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = nn.Sequential(model,\n",
    "                           nn.Softmax(num_classes))\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            scores = model(x) \n",
    "            _, predictions = scores.max(1)\n",
    "            correct += (predictions==y).sum()\n",
    "            total += predictions.size(0)\n",
    "        print(f'Got {correct} / {total} with accuracy {float(correct)/float(total) * 100:.2f}') \n",
    "    \n",
    "    model.train()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9668 / 10000 with accuracy 96.68\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mnist.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
